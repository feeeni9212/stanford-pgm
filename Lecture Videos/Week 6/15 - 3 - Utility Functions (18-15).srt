
1
00:00:01,040 --> 00:00:05,676
When we talked about influence diagram we
included in the influence diagram nodes

2
00:00:05,676 --> 00:00:09,917
that represent the agents' utility
function. And those utility functions we

3
00:00:09,917 --> 00:00:14,045
said indicate an agents' preferences
regarding the state of the world or

4
00:00:14,045 --> 00:00:18,400
different aspects of the state of the
world. What are these utility functions

5
00:00:18,400 --> 00:00:24,971
and where did they come from? So utility
functions are necessary for our ability to

6
00:00:24,971 --> 00:00:30,569
compare, complex scenarios that involve
uncertainty or risk. It's not difficult

7
00:00:30,569 --> 00:00:36,512
for a person to say, that they prefer an
outcome where they get four million to one

8
00:00:36,512 --> 00:00:41,764
where they prefer three million. But it's
not quite as easy to encode a more

9
00:00:41,764 --> 00:00:47,085
complicated, preference that allows us to
compare the utility of these two

10
00:00:47,085 --> 00:00:52,828
lotteries, as they're called. Where the
one on the left gives the agent probabil,

11
00:00:52,828 --> 00:00:58,280
gave the agent $four million with
probability 0.2, and this, the one on the

12
00:00:58,280 --> 00:01:04,479
right gives the agent $three million with
probability 0.25. Which of those lotteries

13
00:01:04,479 --> 00:01:10,005
do we prefer if we had to make that
decision? It turns out that the way to

14
00:01:10,005 --> 00:01:16,353
formalize the decisions making process of
an agent in this type of scenario is by

15
00:01:16,353 --> 00:01:22,402
ascribing a numerical utility to these
different outcomes, to the outcome of four

16
00:01:22,402 --> 00:01:28,550
million. To the outcome 3,000,000 and to
the outcome of $zero. And then we can use

17
00:01:28,550 --> 00:01:34,716
the principle of maximum expected utility
to decide between these two different

18
00:01:34,716 --> 00:01:40,267
lotteries. Specifically we can then
compare 0.2 times the utility of the

19
00:01:40,267 --> 00:01:45,445
outcome 4,000,000 plus. 0.8 to the
authority of the up come zero dollars

20
00:01:45,445 --> 00:01:51,353
versus the converse which is 0.25 versus
the authority, expected authority for the

21
00:01:51,353 --> 00:01:56,566
second lottery to 0.25 time to the
authority of three million dollars plus

22
00:01:56,566 --> 00:02:02,335
0.075 and [inaudible] zero dollars. And if
you compare this to expressions and this

23
00:02:02,335 --> 00:02:07,826
side weather we prefer the one in the
right, the one in the left, the one in the

24
00:02:07,826 --> 00:02:15,409
right or, or there are equally budget in
our view. Now. It might be natural to

25
00:02:15,409 --> 00:02:20,629
assume that utilities should be linear in
the amount of pay off that we get, so that

26
00:02:20,629 --> 00:02:25,784
$five, is preferred about half as much as
$ten. It turns out that that's not the

27
00:02:25,784 --> 00:02:31,132
case for most people and one example of
that is this decision making situation

28
00:02:31,132 --> 00:02:36,886
over here, where on the left the agent has
the option of getting four million dollars

29
00:02:36,886 --> 00:02:42,235
with probability of 0.8, and on the right
they have the option of getting three

30
00:02:42,235 --> 00:02:47,448
million dollars with certainty. Most
people tend to prefer the lottery on the

31
00:02:47,448 --> 00:02:52,737
right, but if one computes they expect
payoff of. These two different lotteries.

32
00:02:52,737 --> 00:03:00,836
We can see that the expected payoff over
here is, is zero, four million times. 0.8

33
00:03:00,836 --> 00:03:09,345
which is 3.2 million. Whereas, on the
right, we have an unexpected payoff of

34
00:03:09,345 --> 00:03:15,120
3,000,000. So, the expected payoff on this
side is higher and nevertheless, people

35
00:03:15,120 --> 00:03:20,525
prefer the lottery on the right. Another
example, very famous example of this type

36
00:03:20,525 --> 00:03:25,580
of, of this type of preference is what
called the Saint Peter's birth paradox.

37
00:03:25,580 --> 00:03:30,508
Saint Peter's birth paradox is, is an
imaginary game that one can play, where a

38
00:03:30,508 --> 00:03:35,752
fair coin is tossed repeatedly until it
comes up head for the first time and if it

39
00:03:35,752 --> 00:03:40,554
comes up head for the first time on the
nth toss you get to [inaudible] nth

40
00:03:40,554 --> 00:03:46,084
dollars. So what's the expected pay off in
this case. Well the probability that is

41
00:03:46,084 --> 00:03:51,453
comes up heads on the first toss is half
and then you get $two. The probability

42
00:03:51,453 --> 00:03:57,640
that is comes up heads for the first as a
quarter and. The payoff here is $four.

43
00:03:58,620 --> 00:04:04,015
Eight, third tosses eight times eight,
times eight dollars and it's easy to see

44
00:04:04,015 --> 00:04:09,686
that the expected payoff over here is
infinite. So in principle people might be

45
00:04:09,686 --> 00:04:15,150
willing to pay any amount to pay, play
this game because they expect the payoff

46
00:04:15,150 --> 00:04:20,753
is bigger than any amount that they would
be paying to play but the fact is that,

47
00:04:20,753 --> 00:04:28,501
for most people. The value of playing this
game. Is approximately $two, which is a

48
00:04:28,501 --> 00:04:34,228
strong indication that their preferences
are not linear in the amount of money that

49
00:04:34,228 --> 00:04:39,803
they earn. So, let's try and quantify that
using this notion, which is called a

50
00:04:39,803 --> 00:04:45,426
utility curve. The utility curve, in this
case, [inaudible] the X axis with the

51
00:04:45,426 --> 00:04:51,342
dollar amount that you get. And on the Y
axis, the utility that an agent describes

52
00:04:51,342 --> 00:04:57,111
to that. And now, let's compare a few
different scenarios here. So first, let's

53
00:04:57,111 --> 00:05:02,982
compare, let's look at the utility of
getting $500. So if we go up from 500 to

54
00:05:02,982 --> 00:05:09,605
the utility car, we consider the utility
of this outcome is going to fall over

55
00:05:09,605 --> 00:05:16,059
here. So, this is going to be the utility
of $500. But now, let's look at, at the

56
00:05:16,059 --> 00:05:23,021
decision, at situation that involves some
risk. So let's look at a set of lotteries

57
00:05:23,021 --> 00:05:29,560
where I get $zero with probability one
minus p, and a $1,000 with probability p.

58
00:05:29,560 --> 00:05:34,689
Because of the linearity of expected
utility, all of these lotteries are going

59
00:05:34,689 --> 00:05:39,490
to sit on this line over here where
depending of the value of p I have a

60
00:05:39,490 --> 00:05:44,620
different weighted combination between
getting the utility of $zero and the

61
00:05:44,620 --> 00:05:49,947
utility of $1000. So for high values of p
close to one we would be sitting on this

62
00:05:49,947 --> 00:05:55,143
side of the curve and otherwise for
example for low values of p we would be

63
00:05:55,143 --> 00:06:01,458
sitting close to here. Specifically what
happens if we look at the probability p

64
00:06:01,458 --> 00:06:07,921
equals 0.5? Well, in that case, we would
have this point on the curve, over here.

65
00:06:07,921 --> 00:06:14,888
Now the important thing to notice is that
the utility of this point where I get

66
00:06:15,140 --> 00:06:20,680
$1,000 probability 50 percent and $zero
probability 50%, that utility in this

67
00:06:20,680 --> 00:06:27,070
example is considerably lower than the
utility of $500. So I prefer. To get the

68
00:06:27,070 --> 00:06:34,342
$500 for certain, which is what most
people will take. Now if we look at. What

69
00:06:34,342 --> 00:06:40,701
the lottery is worth. That is the risky
version. We can see that, that sits over

70
00:06:40,701 --> 00:06:46,164
here and might for example be
corresponding to getting $400 with

71
00:06:46,164 --> 00:06:52,361
certainty. So that $400 is called the
certain equivalent of this lottery over

72
00:06:52,361 --> 00:06:59,128
here. That is, it's the amount that you'd
be willing to trade for this lottery in

73
00:06:59,128 --> 00:07:07,856
terms of getting that money for certain.
The difference. Between. These two

74
00:07:07,856 --> 00:07:14,705
numbers, the expected reward and the
utility of, of that lottery is called the

75
00:07:14,705 --> 00:07:21,554
insurance premium or the risk premium. And
it's called that because that's where

76
00:07:21,554 --> 00:07:28,575
insurance companies make their money.
Because of a person's willingness to take

77
00:07:28,575 --> 00:07:35,424
less money with certainty over a more
risky proposition. So we can see that this

78
00:07:35,424 --> 00:07:43,642
kind of a curve that has this shape, this
concave. Shape, is, is representing a risk

79
00:07:43,642 --> 00:07:50,599
profile which is risk averse. That is, a
person is willing to pay for taking less

80
00:07:50,599 --> 00:07:57,556
risk. Other profiles would, of this, of
this curve, would represent, different

81
00:07:57,556 --> 00:08:04,687
behaviors. So, for example, if the utility
was linear in, in the reward, that would

82
00:08:04,687 --> 00:08:11,966
be a behavior that was called risk
neutral. Conversely if we had a curve that

83
00:08:11,966 --> 00:08:22,500
looked like this, which is a convex
function. That would be risk-seeking.

84
00:08:22,500 --> 00:08:28,634
[sound] And risk seeking behavior occurs
for example in Las Vegas where one is

85
00:08:28,634 --> 00:08:34,774
willing, or in other gambling situations,
where one is willing to actually take a

86
00:08:34,774 --> 00:08:40,840
lost in term of the expected reward for
the small chance of getting a really high

87
00:08:40,840 --> 00:08:47,054
payoff. Now it turns out that people often
have utility curve that looks like the

88
00:08:47,054 --> 00:08:52,602
[inaudible] so if the x axis is the amount
of money that we get and we are

89
00:08:52,602 --> 00:08:59,110
[inaudible] play. It's the zero point over
here, which is at one's current. State.

90
00:08:59,110 --> 00:09:04,917
And we ask, how much do you prefer to earn
money, and how much do you prefer to lose

91
00:09:04,917 --> 00:09:10,444
money? What's your utility for these
different, changes to one's state? We can

92
00:09:10,444 --> 00:09:15,762
see that, one's preferences for earning
money typically exhibit a form of

93
00:09:15,762 --> 00:09:21,134
diminishing returns. [sound]. Which gives
us the, this concave. Utility curve,

94
00:09:21,134 --> 00:09:28,350
which, suggests risk, averse behavior, in
the sense that we would prefer a certain

95
00:09:28,350 --> 00:09:35,483
amount of, we would prefer to get money
with certainty relative to the, expected,

96
00:09:35,734 --> 00:09:42,328
relative to the payoff equivalent,
uncertain lottery. Now on the negative

97
00:09:42,328 --> 00:09:49,553
side of the spectrum many people exhibit
some kind of behavior that is actually

98
00:09:49,553 --> 00:09:56,691
more risk seeking, which means that many
people would prefer a small probability of

99
00:09:56,691 --> 00:10:04,465
a large loss. Relative to a small loss
that you get with certainty, and, that's a

100
00:10:04,754 --> 00:10:12,074
that's a behavior that one often sees.
More importantly in this region of the

101
00:10:12,074 --> 00:10:19,757
space which is close to one's current
state the behavior is often risk neutral

102
00:10:19,757 --> 00:10:26,143
this is small. Small losses or small gains
on the order of a small number of, of

103
00:10:26,143 --> 00:10:31,747
dollars. And of course, it depends on
one's, one's baseline. Are often something

104
00:10:31,747 --> 00:10:37,216
that you don't really don't care about,
having the uncertainty, and the expect-,

105
00:10:37,216 --> 00:10:42,617
expected payoff is often very close to
the, to the utility of the, expected

106
00:10:42,617 --> 00:10:47,829
payoff. Now one final important
observation regarding utility functions is

107
00:10:47,829 --> 00:10:53,841
that one?s utility often depends on many,
many things not just on the monetary gain.

108
00:10:53,841 --> 00:10:59,712
So in all of the attributes that effects
the preferences must be integrated into a

109
00:10:59,712 --> 00:11:05,724
single utility function. This is something
that many people find very painful because

110
00:11:05,724 --> 00:11:11,595
it forces us to do things like put human
life for the loss of human life on the

111
00:11:11,595 --> 00:11:17,290
same scale as, as monetary gain. The point
is even if we don't do this explicitly,

112
00:11:17,290 --> 00:11:22,596
even if we decline to put human life for
example on the same scale as monetary

113
00:11:22,596 --> 00:11:27,566
gain, the fact though our decisions are
indicating that we're making those

114
00:11:27,566 --> 00:11:32,872
decisions. So for example, when an airline
chooses not to run maintenance on the

115
00:11:32,872 --> 00:11:37,681
airplane every single. Time that the air
plane lands that's a financial decision

116
00:11:37,681 --> 00:11:42,449
because that would be too costly but at
the same time it also definitely increases

117
00:11:42,449 --> 00:11:47,283
the chance of loss of human life because
of, because of an accident. Now, it's not

118
00:11:47,283 --> 00:11:51,600
just big companies that make these
decisions. We make these decisions

119
00:11:51,600 --> 00:11:56,605
ourselves. So we don't change the tires on
our cars every month, or every week.

120
00:11:56,605 --> 00:12:01,672
Because that would be too costly. But,
clearly, having better tires is something

121
00:12:01,672 --> 00:12:06,865
that is likely to increase our chances of
surviving an accident or a skid. So these

122
00:12:06,865 --> 00:12:11,938
tradeoffs are ones that we make all the
time, whether we recognize it or not. And

123
00:12:11,938 --> 00:12:18,151
so it's important when we think about a
decision making situation, to list, for

124
00:12:18,151 --> 00:12:24,125
ourselves, all of the different things
that could affect our decision, money,

125
00:12:24,125 --> 00:12:30,696
time pleasure. And many, many other
attributes and think about how we could

126
00:12:30,696 --> 00:12:37,223
bring them together into a single utility
function. Specifically, in the context of

127
00:12:37,223 --> 00:12:42,804
human life, people have spent a lot of
time thinking about how to bring in human

128
00:12:42,804 --> 00:12:48,386
life into one's utility function. And what
turns out to be the wrong strategy, in

129
00:12:48,386 --> 00:12:54,038
terms of reflecting people's preferences,
is to have a utility for the monolithic

130
00:12:54,038 --> 00:12:59,340
event of someone's death. And that turns
out to be a very difficult thing to

131
00:12:59,340 --> 00:13:05,131
contemplate. What, what seems like a
better strategy, in general is this notion

132
00:13:05,131 --> 00:13:13,178
of a micro [inaudible], which is a one in
a million? Chance of death. And so one

133
00:13:13,178 --> 00:13:20,409
puts the risk explicitly into the utility
function. And, and so, what is a one in a

134
00:13:20,409 --> 00:13:27,815
million chance of death worth? Well, back
in 1980, so a while ago, people did, this,

135
00:13:28,076 --> 00:13:35,394
this study. And it turns out that a micro
[inaudible] was worth approximately twenty

136
00:13:35,394 --> 00:13:44,030
of, $twenty in 1980 dollars. And. So of
course you can account for inflation but

137
00:13:44,030 --> 00:13:50,324
it's not a huge amount of money. [sound].
And that turns out to be a much better way

138
00:13:50,324 --> 00:13:56,036
of ranking people's utility for outcomes
that involve risk to human life and asking

139
00:13:56,036 --> 00:14:01,068
about the utility of, of death. The
second, way, that people use a medical

140
00:14:01,068 --> 00:14:06,372
decision, making situations specifically
for accounting for human life, is this

141
00:14:06,372 --> 00:14:11,336
notion of [inaudible], or a quality
adjusted life here. So, each quality

142
00:14:11,336 --> 00:14:16,708
adjusted life here, which is a year
adjusted for one's quality of life, has a

143
00:14:16,708 --> 00:14:22,107
certain, utility associated with it, which
allows it to be compared. With other

144
00:14:22,107 --> 00:14:27,166
aspects that affect our utility in a
decision making situation. One example

145
00:14:27,166 --> 00:14:33,711
from a real world situation, is in this
context of prenatal diagnosis. Where,

146
00:14:33,962 --> 00:14:40,255
researchers did extensive work in
eliciting utility functions that involve

147
00:14:40,255 --> 00:14:46,547
prenatal testing. So the relevant
variables in this scenario include, the,

148
00:14:46,547 --> 00:14:52,756
whether the, baby is going to end up with
some kind of genetic disorder.

149
00:14:52,756 --> 00:15:00,402
Specifically, the one that they focused on
was Down's syndrome. But at the same time,

150
00:15:00,402 --> 00:15:09,020
there's other aspects that affect one's
utility. So For example, V. Pain of.

151
00:15:09,020 --> 00:15:14,538
Testing for down syndrome is one aspect.
The comfort of knowledge that you know

152
00:15:14,538 --> 00:15:20,265
what, what you're going, what's going to
happen is something that also turns out,

153
00:15:20,265 --> 00:15:25,504
out to contribute to one's utility
function. Prenatal testing runs the risk

154
00:15:25,504 --> 00:15:31,022
of the loss of the fetus and that is also
clearly a component of one's utility

155
00:15:31,022 --> 00:15:36,233
function. And at. At the same time, the
potential for future pregnancy. That is

156
00:15:36,233 --> 00:15:41,302
whether there will be a future pregnancy
or not, is another component of one's

157
00:15:41,302 --> 00:15:46,630
utility function. So if we think about the
space here, the utility function depends

158
00:15:46,630 --> 00:15:52,748
in a complicated way of a large number On,
on these five variables and this is fairly

159
00:15:52,748 --> 00:15:58,705
high dimensional space over which to
illicit to illicit utilities. Fortunately

160
00:15:58,924 --> 00:16:04,979
it turns out that many people have a lot
of structure in your utility function. And

161
00:16:04,979 --> 00:16:10,670
specifically they can breakdown the
totality function as a sum of [inaudible],

162
00:16:10,670 --> 00:16:15,995
just as we had in the context of
[inaudible] diagram. And for many people

163
00:16:15,995 --> 00:16:21,517
that the composition looks like the
utility of the testing. The, a separate

164
00:16:21,517 --> 00:16:28,193
component for the utility of the peace of
mind of knowledge. And then we have. These

165
00:16:28,193 --> 00:16:33,060
two pair wise utility terms, the first of
which is a term that depends

166
00:16:33,060 --> 00:16:38,773
simultaneously on Down's syndrome, and on
the loss of the fetus. And the second is,

167
00:16:38,984 --> 00:16:44,557
the utility that depends on the loss of
the fetus, and the potential for future

168
00:16:44,557 --> 00:16:50,270
pregnancy. So people's utility function,
for many people, decomposes in this way.

169
00:16:50,270 --> 00:16:55,772
Which it turns out we could actually.
Think about it as a graphical model that

170
00:16:55,772 --> 00:17:01,697
has singleton terms as well as these pair
wise terms over here. And, that allows us

171
00:17:01,697 --> 00:17:07,482
to considerably reduce the number of terms
that we need to list in order to get a

172
00:17:07,482 --> 00:17:13,370
usable utility function. [sound] So to
summarize our utility function is what we

173
00:17:13,370 --> 00:17:18,354
can use to determine preferences about
decisions that involve risk or

174
00:17:18,354 --> 00:17:24,477
uncertainty. In order to define or elicit
the utility function, we generally need to

175
00:17:24,477 --> 00:17:35,710
consider multiple factors all of which
affect our utility. In most cases, the

176
00:17:35,710 --> 00:17:42,707
relationship between these different
factors, the. Between say money and the

177
00:17:42,707 --> 00:17:48,031
utility or micromores and the utility.
This relationship is usually a nonlinear

178
00:17:48,031 --> 00:17:53,160
one. And the shape of the utility curve
determines one's attitudes towards risk.

179
00:17:53,160 --> 00:17:59,124
Finally the actual utility function is
usually a multi-attribute utility that

180
00:17:59,124 --> 00:18:04,720
integrates all these different factors,
and it often helps to decompose this

181
00:18:04,720 --> 00:18:10,316
utility function into tractable pieces,
often as a sum of these pieces, which

182
00:18:10,316 --> 00:18:15,471
allows us to make this [inaudible] problem
much more manageable.
