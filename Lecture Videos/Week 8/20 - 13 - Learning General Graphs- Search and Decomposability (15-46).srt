
1
00:00:00,000 --> 00:00:04,304
We talked about how the problem of
learning a general base net structure can

2
00:00:04,304 --> 00:00:08,833
be viewed as a common coil optimization
problem in which we are trying to find a

3
00:00:08,833 --> 00:00:13,194
high scoring network structure. And how
that's problem, that problem is usually

4
00:00:13,194 --> 00:00:17,778
solved by some kind of heuristic search
over a space of base net structures. Let's

5
00:00:17,778 --> 00:00:22,195
talk now about the computational cost of
this algorithm and how we can use the

6
00:00:22,195 --> 00:00:26,723
property of decomposability, which we also
previously used in the context of tree

7
00:00:26,723 --> 00:00:30,860
learning to considerably reduce the
computational cost of this of this

8
00:00:30,860 --> 00:00:35,147
algorithm. So as a reminder, the
[inaudible] search procedure that we

9
00:00:35,147 --> 00:00:40,287
discussed, iteratively moves over the
space of networks. And so if this is the

10
00:00:40,287 --> 00:00:45,492
network that we're currently at, we need
to evaluate several different moves away

11
00:00:45,492 --> 00:00:50,504
from that network. That's [inaudible]
usually local ways by adding the leading

12
00:00:50,504 --> 00:00:56,071
or reversing an arc. We then score each of
these subsequent networks. And see which

13
00:00:56,071 --> 00:01:01,458
of those has the highest score. And take
that move. And one can also have other

14
00:01:01,458 --> 00:01:07,259
algorithms that evaluate the, the, the
[inaudible] that make the greedy move at

15
00:01:07,259 --> 00:01:12,870
each point. But the basic idea is the
same. So what is the computational cost of

16
00:01:12,870 --> 00:01:19,285
this algorithm? Let's do the naive
computational analysis. What we would get

17
00:01:19,285 --> 00:01:26,203
if we were to do the naive implementation
of this. So, how many operators are there

18
00:01:26,203 --> 00:01:32,783
in each search step that we need to
evaluate? So, in a Bayesian network with,

19
00:01:33,037 --> 00:01:43,284
N nodes, we have, N times N - one,
different possible edges. One now each of

20
00:01:43,284 --> 00:01:50,033
those edges is either present in the
graph, or not present in the graph. If the

21
00:01:50,033 --> 00:02:01,346
edge is present in the graph, we can
either delete it. Or reverse it. And if

22
00:02:01,346 --> 00:02:08,205
it's absent. We can add it. Which means
that effectively we have either two or one

23
00:02:08,205 --> 00:02:13,797
possibilities for each of those N times N
-one possible edges and so we have O of N

24
00:02:13,797 --> 00:02:19,465
squared possible operators that we need to
evaluate at each point in this step. Each,

25
00:02:19,465 --> 00:02:24,992
each step in the algorithm. What is the
cost of evaluating each of the candidate

26
00:02:24,992 --> 00:02:30,865
successors that we would get by taking one
of these operators? So reminding ourselves

27
00:02:30,865 --> 00:02:36,255
that there are multiple components in this
score, one for each variable in the

28
00:02:36,255 --> 00:02:41,608
network because of the decomposability
property, so we have n. Different

29
00:02:41,608 --> 00:02:49,270
components. And for each of those we have
to look at this efficient statistics and

30
00:02:49,270 --> 00:02:55,702
compute the resulting entry in the score
corresponding to a variable. Computing

31
00:02:55,702 --> 00:03:01,743
efficient statistics requires a traversal
over the training data. And so that

32
00:03:01,743 --> 00:03:08,098
something would takes o of m time, where m
is the number training existence. So

33
00:03:08,098 --> 00:03:14,322
altogether, this stat requires o and times
m. Now, we haven't talked about this, but

34
00:03:14,322 --> 00:03:20,168
one also needs to make sure that the
resulting graph from this operator is in

35
00:03:20,168 --> 00:03:25,939
fact acyclic. And, so we need to do an
acyclicity check. This is something that

36
00:03:25,939 --> 00:03:31,710
in general requires O of little M time
where M is the number of edges in the

37
00:03:31,710 --> 00:03:40,618
graph. So altogether, if we sum up all of
these different pieces of the cost, we end

38
00:03:40,618 --> 00:03:48,017
up with, a computational cost, which is O
of N squared times MN+little m. Where, by

39
00:03:48,017 --> 00:03:55,527
and large, little m is usually dwarfed by,
M, by big M times N. And that is the cost

40
00:03:55,527 --> 00:04:00,644
per [inaudible] up. Now if you think of
that work, it?s not even the [inaudible]

41
00:04:00,644 --> 00:04:05,826
that large something that has fifty or
hundred variables so that ends fifty to

42
00:04:05,826 --> 00:04:10,680
hundred and the number of training
instances is ten thousand this can get

43
00:04:10,680 --> 00:04:15,994
really-really large and generally
impractical to do in a lot of situations.

44
00:04:15,994 --> 00:04:21,733
So how to improve on this. Let's see how
we can exploit the decomposability

45
00:04:21,733 --> 00:04:26,412
property to get a significant
computational savings in the search

46
00:04:26,412 --> 00:04:32,084
process. Let's first look at a single
small operator, such as the one where we

47
00:04:32,084 --> 00:04:37,458
have an edge between B and D. To this
network where such an edge did not exist

48
00:04:37,458 --> 00:04:43,013
before. And let's consider the score that
we had for the original network which, in

49
00:04:43,013 --> 00:04:48,499
this case is because of decomposability
property, is the sum of family scores for

50
00:04:48,499 --> 00:04:53,985
the individual variables. So we have a
component that lists a score of A relative

51
00:04:53,985 --> 00:04:59,607
to its empty set of parents, the score of
B relative to the empty set, C relative to

52
00:04:59,607 --> 00:05:05,709
its parents A and B and D relative to its
parents C. Let's compare that to the score

53
00:05:05,709 --> 00:05:11,484
of the network following the move. And we
can see that this score for the new

54
00:05:11,484 --> 00:05:16,990
network is actually very similar to the
score of the original network, because.

55
00:05:16,990 --> 00:05:22,488
For the same decomposability property, we
can break up the score into these little

56
00:05:22,488 --> 00:05:27,340
components. And since most families
haven't changed that component, then the

57
00:05:27,340 --> 00:05:31,545
score's going to be the same.
Specifically, we're going to have the

58
00:05:31,545 --> 00:05:36,527
exact same score for A, relative to its
empty parent set. The same score for B,

59
00:05:36,527 --> 00:05:41,961
the same score for C. And only this, only
this last score for D is now going to be

60
00:05:41,961 --> 00:05:47,115
different, because of. The fact that we've
modified the family for D. But that

61
00:05:47,115 --> 00:05:52,613
immediately suggest that we don't really
need to recompute these earlier components

62
00:05:52,613 --> 00:05:58,176
of the score because they haven't changed.
We only need to compute the last component

63
00:05:58,176 --> 00:06:02,361
corresponding to D. In fact we can do
better than that by, instead of

64
00:06:02,361 --> 00:06:07,284
considering the absolute square. Instead
we are going to compute what's called the

65
00:06:07,284 --> 00:06:11,547
delta square, which is the difference
between the square of the network

66
00:06:11,547 --> 00:06:16,170
following the change, this network, and
the square of the original network. And

67
00:06:16,170 --> 00:06:20,973
we're going to compute the difference
between those two squares, which as we can

68
00:06:20,973 --> 00:06:26,136
see, is just the difference between the
squares of the two families for D the BC

69
00:06:26,136 --> 00:06:30,339
family in the following, in the new
network versus the C family in the

70
00:06:30,339 --> 00:06:37,255
original network. And that delta score is
going to be something that we can compute

71
00:06:37,255 --> 00:06:43,211
just looking at a single family. It can
ruin the rest of the network. The same

72
00:06:43,211 --> 00:06:48,507
thing happens for other local operators.
So, for example, if we consider now, the

73
00:06:48,507 --> 00:06:54,007
deletion operator, that deletes an edge
between B and C. The, and we look at the

74
00:06:54,007 --> 00:06:59,507
delta score, that delta score only cares
about the, the family C, because that's

75
00:06:59,507 --> 00:07:04,667
the only family to have changed. And
that's going to be, again the difference

76
00:07:04,667 --> 00:07:10,235
between the score of C with a single, with
a single edge A, minus the score of C,

77
00:07:10,235 --> 00:07:16,619
with a family AD. So again, only one
family gets affected by this change. For

78
00:07:16,619 --> 00:07:21,938
an edge reversal, the situation's a little
bit more complicated because we can see

79
00:07:21,938 --> 00:07:27,386
that by flipping the edge from B to C and
making it go from C to B, there's actually

80
00:07:27,386 --> 00:07:32,445
two families that are affected, the B
family and the C family. But that's still

81
00:07:32,445 --> 00:07:37,764
just two as opposed to the entire network.
And so, we can see that, that delta score

82
00:07:37,764 --> 00:07:42,953
is going to have two components. One is
the delta score for the C family, and the

83
00:07:42,953 --> 00:07:47,947
second is the delta score for the B
family. But in either case, we only end up

84
00:07:47,947 --> 00:07:53,011
affecting either one. Family, in the case
of edge addition, or in that case, a case

85
00:07:53,011 --> 00:07:58,137
of edge deletion, or utmost, two families
in the case of edge reversal. And so that

86
00:07:58,137 --> 00:08:03,516
means that we only have to consider a much
smaller fraction of the net worth when

87
00:08:03,516 --> 00:08:09,509
computing the delta square. A second use
of this possibility property comes up when

88
00:08:09,509 --> 00:08:15,508
we consider multiple consecutive steps of
the search algorithm. So let's imagine

89
00:08:15,508 --> 00:08:21,507
that in the previous step, we decided to
take that steps that added the edge for

90
00:08:21,507 --> 00:08:27,581
[inaudible] and now we have to consider
the next step in the search. But what are

91
00:08:27,581 --> 00:08:33,805
the operators that are conceivable in this
next step of the search. For example, one

92
00:08:33,805 --> 00:08:40,024
such operator is to delete the edge from
[inaudible]. This one, and notice that,

93
00:08:40,024 --> 00:08:45,798
that edge deletion operation is in fact
operator that we considered in previous

94
00:08:45,798 --> 00:08:51,641
step of the search before we decided to
add the edge from B to D. Now not only is

95
00:08:51,641 --> 00:08:57,401
this operator the same. Notice that the
family of C hasn't changed between those,

96
00:08:57,617 --> 00:09:03,232
between those two cases. And both of
these, cases, when we're considering the

97
00:09:03,232 --> 00:09:09,135
move, C has the parents of A and B. And so
the delta score for that particular move

98
00:09:09,135 --> 00:09:14,102
is not going to change either.
Specifically, if in this case, the delta

99
00:09:14,102 --> 00:09:20,150
score was, the score of C, given family A,
minus the score of C, given the family AV.

100
00:09:20,150 --> 00:09:27,464
We have exactly that same score. The same
delta score in, in the previous step. That

101
00:09:27,464 --> 00:09:32,904
is, these two delta scores in the previous
step and the new step are exactly the

102
00:09:32,904 --> 00:09:39,886
same. And so there's really no point to
recompute it. So which scores do we need

103
00:09:39,886 --> 00:09:45,394
to recompute? The only scores that we need
to recompute are the ones that were

104
00:09:45,394 --> 00:09:50,973
affected by the step that we currently,
that we just took. So specifically if we

105
00:09:50,973 --> 00:09:56,905
took a step that modified the family of D,
then any step that involves an additional

106
00:09:56,905 --> 00:10:02,484
change to the family of D will have a
different delta score because the family

107
00:10:02,484 --> 00:10:08,575
is now different in doing the comparison.
However, families that were not affected

108
00:10:08,575 --> 00:10:15,047
by the move don't need to be recomputed.
So to summarize, we only need to re-score

109
00:10:15,047 --> 00:10:21,908
Delta moves, Delta scores, for families
that changed in the last move. So let's

110
00:10:21,908 --> 00:10:27,630
summarizes the computational cost of this
procedure. What's the cost per move?

111
00:10:27,630 --> 00:10:33,526
Having decided to take a move we have only
one or two families that were affected by

112
00:10:33,526 --> 00:10:40,164
that move. That means that. Only O and
delta scores need to be computed because

113
00:10:40,164 --> 00:10:44,961
for a given family there is only N
possible edges that impends on that

114
00:10:44,961 --> 00:10:50,638
family. So only O and delta scores need to
be computed and for each of those we need

115
00:10:50,638 --> 00:10:56,408
to compute sufficient statistics, which
takes OM time. So all together we have O

116
00:10:56,408 --> 00:11:03,481
of little m times m as the cost of doing
this stuff which is actually two orders of

117
00:11:03,481 --> 00:11:09,879
magnitude better than old n cube times m
that we had for the original naive

118
00:11:09,879 --> 00:11:17,998
computation. Now this tells us the cost
after we pick a move. What about the cost

119
00:11:17,998 --> 00:11:22,869
of picking the [inaudible]? Now, naively
we might say that there's N squared

120
00:11:22,869 --> 00:11:28,259
possible operators that we can consider
any given move so we need to evaluate each

121
00:11:28,259 --> 00:11:33,714
of them and pick the best one, or consider
each of them and pick the best one, but in

122
00:11:33,714 --> 00:11:39,169
fact we can do considerably better by the
use of clever data structure, specifically

123
00:11:39,169 --> 00:11:44,040
we can maintain a priority queue of
operators sorted by their delta scores.

124
00:11:44,040 --> 00:11:51,407
Now when we re-score those O-event
operators. In this step over here. We need

125
00:11:51,407 --> 00:11:57,060
to modify the score of those operators,
and reinsert them into the priority cue in

126
00:11:57,060 --> 00:12:02,506
their appropriate place. But that's a
computation that requires [inaudible] and

127
00:12:02,506 --> 00:12:08,090
login time, because there's only N of
them. And once we have done that, the best

128
00:12:08,090 --> 00:12:14,088
operator will be at the top of the list.
Which we can then, take, identify and take

129
00:12:14,088 --> 00:12:20,020
in constant time. And so this priority
queue saves us complexity by taking us

130
00:12:20,020 --> 00:12:26,215
from old S square time for picking this
for traversing the set of operators to

131
00:12:26,215 --> 00:12:32,411
something that requires [inaudible] log
in. And so altogether we've actually saved

132
00:12:32,411 --> 00:12:39,304
a considerable cost in both of these
steps. It turns out that one can, get an

133
00:12:39,304 --> 00:12:44,709
even higher level of computational
efficiency, based on a different

134
00:12:44,709 --> 00:12:51,324
observation. So, it's a fact that, in most
network learning algorithms, the plausible

135
00:12:51,324 --> 00:12:57,616
families, the ones that have some chance
of being selected, are variations on a

136
00:12:57,616 --> 00:13:03,827
theme. That is, for a given variable A,
there might be some number of variables.

137
00:13:04,069 --> 00:13:12,265
You know, B1, B2. Up to DK for a very
small K. That are reasonable parents to be

138
00:13:12,265 --> 00:13:19,763
selected as parents for A. And so. How do
we exploit that property in computational,

139
00:13:19,763 --> 00:13:25,209
to get computational savings? Turns out
there's two different, ways that one can

140
00:13:25,209 --> 00:13:30,654
utilize this. The first is the fact that,
because we have the same set of variables

141
00:13:30,654 --> 00:13:35,767
being constantly considered as being
candidate families. It means that we can

142
00:13:35,767 --> 00:13:41,412
use sufficient statistics that we computed
in one step, and reuse them in a different

143
00:13:41,412 --> 00:13:46,525
step if we cache them. Because the,
because we're likely to encounter the same

144
00:13:46,525 --> 00:13:51,849
family more than once. We might encounter
B as a parent of A. And A as a pair, as a

145
00:13:51,849 --> 00:13:57,220
possible pairing for B and for both of
these we have the sufficient statistics. A

146
00:13:57,220 --> 00:14:02,501
B that are going to be utilized for both
of them. And so, if we compute this once

147
00:14:02,501 --> 00:14:07,650
and then cache it, these sufficient
statistics we don't have to recompute them

148
00:14:07,650 --> 00:14:12,601
again. That turns out to be a huge
difference in terms of the computational

149
00:14:12,601 --> 00:14:19,436
cost of this algorithm. The second way
which this could be used is that if we can

150
00:14:19,436 --> 00:14:26,071
identify in advance the set of B1 up to BK
that are reasonable parents to consider

151
00:14:26,071 --> 00:14:31,604
for A. We can restrict in advance. That
set, and not consider other parents at

152
00:14:31,604 --> 00:14:36,989
all, which reduces our complexity from o
of n to o of k, where k is some bound on

153
00:14:36,989 --> 00:14:42,441
the number of plausible parents that we're
willing to consider. Now this, now is a

154
00:14:42,441 --> 00:14:47,894
heuristic in the sense that this is a
restriction that could actually change the

155
00:14:47,894 --> 00:14:53,480
outcome of our algorithm. It's not just a
way of reducing the cost, but it turns out

156
00:14:53,480 --> 00:14:59,096
to be a very good approximation and
practice. To summarize, it turns out that

157
00:14:59,096 --> 00:15:05,334
even the fairly simple heuristic structure
search that we employ in, in, such as

158
00:15:05,334 --> 00:15:11,116
greedy hill climbing, can get expensive
when N is large. Because the, naive

159
00:15:11,116 --> 00:15:17,506
implementation has the cubic dependence on
N. But we can exploit the decomposability

160
00:15:17,506 --> 00:15:23,363
property, that we also exploit in the
context of tree learning, to get several

161
00:15:23,363 --> 00:15:29,713
orders of magnitude reduction in the cost
of this search. We can also exploit the

162
00:15:29,713 --> 00:15:36,342
recurrence of plausible families at
multiple steps in the search algorithm to

163
00:15:36,342 --> 00:15:42,971
get further computational savings and also
restrict the search space to, to get

164
00:15:42,971 --> 00:15:46,076
better, better computational property
