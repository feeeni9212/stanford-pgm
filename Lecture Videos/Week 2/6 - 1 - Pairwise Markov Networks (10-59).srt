
1
00:00:02,081 --> 00:00:06,054
So as we said at the very beginning,
there's two main families of graphical

2
00:00:06,054 --> 00:00:10,017
models. There's those that are based on
directed graphs, directed acyclic

3
00:00:10,017 --> 00:00:13,055
graphs, and those that are based on
undirected graphs. The undirected

4
00:00:13,055 --> 00:00:17,017
graphical models are typically called
Markov networks. They're also called

5
00:00:17,017 --> 00:00:21,040
Markov random fields. We're going to start
by talking about the simplest sub class of

6
00:00:21,040 --> 00:00:25,012
those, which is pairwise Markov
networks, and then we're gonna generalize

7
00:00:25,012 --> 00:00:29,084
it. So let's look again at a toy example
just to illustrate what's going on. This

8
00:00:29,084 --> 00:00:34,058
is an example of four people who are
studying together in study pairs. And

9
00:00:34,058 --> 00:00:39,049
notice that Alice and Charles don't get
along. And, you know, Bob and Debbie had a

10
00:00:39,049 --> 00:00:44,035
bad breakup so they don't talk to each
other either. And so really we only have

11
00:00:44,035 --> 00:00:48,079
the study pairs that are marked by the
edges on this diagram. The random

12
00:00:48,079 --> 00:00:54,059
variables here indicate whether the
students have a particular misconception

13
00:00:54,059 --> 00:01:00,046
because the material was a little confusing. So the
random variable says does the student

14
00:01:00,046 --> 00:01:06,011
have a misconception or not, this
particular misconception. The intuition

15
00:01:06,011 --> 00:01:12,021
here says if two students study together
then they kind of influence each other. So we

16
00:01:12,021 --> 00:01:18,023
have for example if Alice and Bob study
together then this edge indicates that if

17
00:01:18,023 --> 00:01:22,024
one of them has the misconception, the
other one is likely to have the

18
00:01:22,024 --> 00:01:26,042
misconception. Now, this doesn't fit
neatly into the purview of the directed

19
00:01:26,042 --> 00:01:30,082
graph, because the influence flows in both
directions. You can't really point an

20
00:01:30,082 --> 00:01:35,005
arrow from Alice to Bob or from Bob to
Alice. So we're going to use an undirected

21
00:01:35,005 --> 00:01:39,080
graph to represent this. So that's great,
but how do you parametrize an undirected

22
00:01:39,080 --> 00:01:44,017
graph? Because you no longer have the
notion of a conditional probability

23
00:01:44,017 --> 00:01:48,085
distribution, because there's no variable
that's conditioning and one that you

24
00:01:48,085 --> 00:01:53,088
condition on. And so, and so, how do we do
this? And we're, so we're going to use the

25
00:01:53,088 --> 00:01:58,043
general notion of a factor, which we
defined previously. And notice that this

26
00:01:58,043 --> 00:02:03,029
really is a general factor, in that the
numbers don't even, are not even within

27
00:02:03,029 --> 00:02:08,022
the range [0, 1]. Now, what do these factors
mean? These factors have many names;

28
00:02:08,022 --> 00:02:15,087
they're called affinity functions or
compatibility functions. They're also,

29
00:02:15,087 --> 00:02:26,036
they're also called soft constraints in
different settings. So what these numbers

30
00:02:26,036 --> 00:02:33,025
mean is the oh, is the local happiness of
the variables A and B to take a particular

31
00:02:33,025 --> 00:02:39,065
joint assignment. So here we have that,
you know, we can see the that the happiest

32
00:02:39,065 --> 00:02:45,039
assignment as far as A and B are
concerned, in isolation of everything

33
00:02:45,039 --> 00:02:50,076
else, is a0 b0. Okay. This is the case
where neither student has the

34
00:02:50,076 --> 00:02:55,006
misconception. That's the happy
assignment. We see that the second

35
00:02:55,006 --> 00:03:00,029
happiest assignment is a a1, b1 where
again the students agree and in this case

36
00:03:00,029 --> 00:03:05,039
they both have the misconception. And
finally, the other two in the middle are,

37
00:03:05,059 --> 00:03:11,031
are the least happy of all. Now this is a
local happiness, and we have similar notions

38
00:03:11,031 --> 00:03:17,022
of happiness for the other pairs in the
graph. So in this case, we see not only that there is

39
00:03:17,022 --> 00:03:22,091
there a strong sentiment in favor of
agreement, it's much stronger than in the

40
00:03:22,091 --> 00:03:28,045
AB case. So B and C really like to agree
with each other. They you know, it is

41
00:03:28,045 --> 00:03:34,060
very difficult for them to have opposing
opinions. Okay, on the other hand Charles

42
00:03:34,060 --> 00:03:40,000
and Debbie like to argue with each other
all the time, and so if one of them says it's

43
00:03:40,000 --> 00:03:45,005
going to rain today, the other one is
going to say that its sunny today. And so

44
00:03:45,005 --> 00:03:50,058
really you can see that the preferred
assignment for their local opinion is the

45
00:03:50,058 --> 00:03:56,005
ones that they disagree with each
other. Okay? And again A and B like to

46
00:03:56,005 --> 00:04:01,027
agree. So this is sort of a you know
describing the overall state by a bunch of

47
00:04:01,027 --> 00:04:06,008
little pieces and how we are gonna put
these pieces together to define a joint

48
00:04:06,008 --> 00:04:11,033
probability distribution. We are going to
use the notion of product of factors and

49
00:04:11,033 --> 00:04:16,008
so here we are and we are going to take
all these factors and we are gonna

50
00:04:16,008 --> 00:04:20,091
multiply them together. That's great.
Except that there's, this is in no way,

51
00:04:20,091 --> 00:04:26,026
shape, or form a probability distribution,
because it's numbers aren't even in the

52
00:04:26,026 --> 00:04:31,028
interval [0, 1]. Which is why, you'll
notice there's a little tilde on top of

53
00:04:31,028 --> 00:04:42,078
the P. This tilde [indicates un]normalized measure.
Okay? So how do we turn an un-normalized

54
00:04:42,078 --> 00:04:47,077
measure into a probability distribution?
Well, we normalize it. Well actually,

55
00:04:47,077 --> 00:04:52,024
sorry. Before that, here is the
un-normalized measure. So you can see it

56
00:04:52,024 --> 00:04:57,074
here, this, just to sort of highlight the
point, how do we turn this un-normalized

57
00:04:57,074 --> 00:05:02,079
measure into a probability distribution?
We normalize it. And that normalization

58
00:05:02,079 --> 00:05:06,064
here has a name. It's called the
partition function for historical

59
00:05:06,064 --> 00:05:10,079
reasons that come from its origins in
statistical physics, and I'm not even

60
00:05:10,079 --> 00:05:15,032
gonna describe why it's called that, but
that's what it's called. But you can think

61
00:05:15,032 --> 00:05:19,091
of it simply as the normalizing constant
that is going to make all of these sum to

62
00:05:19,091 --> 00:05:24,012
one. So we're gonna get it by simply
summing up all these entries, and that's

63
00:05:24,012 --> 00:05:28,059
going to give us the value Z. And if we
divide all of these entries by Z, we get a

64
00:05:28,059 --> 00:05:32,085
normalized probability distribution. And
that is the probability distribution

65
00:05:32,085 --> 00:05:39,060
that's defined by this graph. So now let's
think about what these factors mean. And

66
00:05:39,060 --> 00:05:45,025
let's think about this factor phi1 of AB,
which is this local happiness between A

67
00:05:45,025 --> 00:05:50,054
and B. And let's think about how it
relates the probability distribution. So

68
00:05:50,054 --> 00:05:56,012
we might think that, this is the marginal
probability of A and B in the joint

69
00:05:56,012 --> 00:06:01,021
distribution. Or maybe it's the
conditional distribution of A given B, or

70
00:06:01,021 --> 00:06:07,076
maybe B given A. Or maybe it's a joint
probability of A and B given C or D. The

71
00:06:07,076 --> 00:06:14,044
answer is, it's none of the above. So,
let's go back and look at what this

72
00:06:14,044 --> 00:06:22,059
actually means in this particular context.
So here we have the set of factors that we

73
00:06:22,059 --> 00:06:29,030
use to construct this distribution. And
here, trust me, is the marginal, marginal

74
00:06:29,030 --> 00:06:38,006
probability of A and B. As defined by the
set of factors PHI. We're going to use a

75
00:06:38,006 --> 00:06:43,028
little PHI here to denote the fact that
it was derived from the set of

76
00:06:43,028 --> 00:07:02,095
factors PHI equals phi1 ... phi n. Oh come on.
[inaudible]. Again phi1, 2, phi3 ...

77
00:07:04,057 --> 00:07:13,004
Okay. So lets compare this
distribution, to the factor phi1. We

78
00:07:13,004 --> 00:07:19,063
can see that not only does it not respect
the fact that A and B like to agree with

79
00:07:19,063 --> 00:07:26,014
each other. Here, A and B like to agree
with each other by a lot. I mean, remember

80
00:07:26,014 --> 00:07:32,026
this is the, this is three times higher
than the next highest value. Here, that

81
00:07:32,026 --> 00:07:38,053
probability has 0.13. And the other high
value assignment in, on this side the

82
00:07:38,053 --> 00:07:44,080
one that had ten, has only 0.04. What is
the single highest assignment here? It's

83
00:07:44,080 --> 00:07:52,077
this one. Let's think about why that is.
This probabiity distribution is constructed

84
00:07:52,077 --> 00:07:58,023
by multiplying all four of these factors.
And if you think about what's going on

85
00:07:58,023 --> 00:08:03,075
here, you see that B really, really likes
to agree with C. So these guys are really

86
00:08:03,075 --> 00:08:12,050
closely tied together. And -- actually this
should probably be [inaudible] -- and A and D similarly

87
00:08:12,050 --> 00:08:20,030
like to agree. So these are really, really
closely tied together. These guys, C and

88
00:08:20,030 --> 00:08:27,091
D, strongly like to disagree. They like to
have opposite values. Now all three of

89
00:08:27,091 --> 00:08:33,046
these factors are actually stronger. That
is the differences between the assignments

90
00:08:33,046 --> 00:08:37,072
are bigger, than in phi1. So where are you
gonna break the cycle. You can't have D

91
00:08:37,072 --> 00:08:41,094
agreeing with A, A agreeing with B, B
agreeing with C and C disagreeing with D,

92
00:08:41,094 --> 00:08:46,043
doesn't work. And so somewhere this cycle
has to be, this loop has to be broken and

93
00:08:46,043 --> 00:08:50,098
the place where it gets broken is A and B
because it's a weaker factor. So the

94
00:08:50,098 --> 00:08:55,014
A and B probability is actually some kind
of complicated aggregate of these

95
00:08:55,014 --> 00:08:59,092
different factors that are used to compose
the Markov network. And this is actually

96
00:08:59,092 --> 00:09:05,042
an important point because it is going to
come back and haunt us in later parts of

97
00:09:05,042 --> 00:09:10,085
the course. There isn't a natural mapping
between the probability distribution and

98
00:09:10,085 --> 00:09:16,028
the factors that are used to compose it.
You can't look at the probability distribution and say

99
00:09:16,028 --> 00:09:21,058
aha, this piece of it is what phi1 ought to be.
This is in direct contrast to Bayesian

100
00:09:21,058 --> 00:09:25,041
network where the [factors] were all
conditional probabilities, and you could

101
00:09:25,041 --> 00:09:29,030
just look at the distribution and compute
them, here you can't do [that]. And, that,

102
00:09:29,030 --> 00:09:33,039
actually turns out to affect things, like
how we can learn these factors from

103
00:09:33,039 --> 00:09:36,084
data, because you can't just extract them
directly from the probability

104
00:09:36,084 --> 00:09:42,030
distribution. So with that definition we
can, with that intuition, we can now go

105
00:09:42,030 --> 00:09:47,007
ahead and define a pairwise Markov
network and I'm defining it exclusively

106
00:09:47,007 --> 00:09:52,001
because pairwise Markov networks are
sufficiently commonly used as a class of

107
00:09:52,001 --> 00:09:57,026
general Markov networks that, that it's
worth giving them they're own place. So a

108
00:09:57,026 --> 00:10:02,094
pairwise Markov network is an
undirected graph whose nodes are the random

109
00:10:02,094 --> 00:10:13,023
variables X1 up to Xn. And we
have edges Xi connecting to Xj and

110
00:10:13,023 --> 00:10:20,043
each one of them is associated with a
factor, also known as a potential, phi i j,

111
00:10:20,043 --> 00:10:27,062
oops, Xi [inaudible] [X]j, okay. That's what -- this shouldn't be
an edge, this should be a comma. That's a

112
00:10:27,062 --> 00:10:33,090
pairwise Markov network and from that.
And here is an example of slightly

113
00:10:33,090 --> 00:10:39,006
larger Markov network, this is a Markov
network that is in the form of a grid and

114
00:10:39,006 --> 00:10:43,074
this is the kind of network that's used
for example when we are doing various

115
00:10:43,074 --> 00:10:48,042
operations on images because then the
variables correspond to pixels for example.

116
00:10:48,064 --> 00:10:54,055
And this is the Markov network that
corresponds to image the segmentation when

117
00:10:54,055 --> 00:10:59,048
we're using super pixels. In which case
it's no longer a regular grid.
