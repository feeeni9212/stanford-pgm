
1
00:00:00,000 --> 00:00:04,417
So far we talked about a more of and
[inaudible] of learning clearly in the

2
00:00:04,417 --> 00:00:08,893
context of maximum or clear destination.
Where our goal is to optimize likely

3
00:00:08,893 --> 00:00:13,136
dis-function or the congetional
dis-function. But as for Bayesian network

4
00:00:13,136 --> 00:00:17,612
maximum at your destination is not a
particularly good regiment and its very

5
00:00:17,612 --> 00:00:21,856
susceptible to the over fining of the
parameters to the specifics of the

6
00:00:21,856 --> 00:00:26,568
training data. So what we'd like to do, is
we'd like to utilize some ideas that we

7
00:00:26,743 --> 00:00:31,864
exploited also in the context of Bayesian
networks which are ideas such as parameter

8
00:00:31,864 --> 00:00:36,519
priors to smooth out our estimates of the
parameters at least in initial phases

9
00:00:36,519 --> 00:00:41,000
before we have a lot of data to really
drive us into the right region of the

10
00:00:41,000 --> 00:00:45,752
space. So, in the context of [inaudible]
networks, that was all great, because we

11
00:00:45,752 --> 00:00:50,521
could have a conjugate prior on the
parameters, such as a [inaudible] prior

12
00:00:50,521 --> 00:00:55,596
that we could then integrate with the
likelihood to obtain a closed [inaudible]

13
00:00:55,596 --> 00:01:00,230
conjugate posterior, and it was all very
computationally elegant. But in the

14
00:01:00,230 --> 00:01:05,206
context if MRF's and CRF's, even the
likelihood itself is not computationaly

15
00:01:05,206 --> 00:01:10,575
elegant and can?t be maintained in closed
form. And therefore the posterior, is also

16
00:01:10,575 --> 00:01:15,159
on something that's going to become
computationaly elegant. And so the

17
00:01:15,159 --> 00:01:20,201
question is how do we then incorporate
ideas such as priors into MRF and CRF

18
00:01:20,201 --> 00:01:25,668
learning, so as to get some of the benefit
of regularization. So the idea here in

19
00:01:25,668 --> 00:01:31,670
this context, is to use what's called map
estimation. Where we have a prior, but

20
00:01:31,670 --> 00:01:37,826
instead of maintaining a posterior in
closed form, we're computing what's called

21
00:01:37,826 --> 00:01:43,011
the maximum posteriori estimate. The
parameters. And this in fact is same

22
00:01:43,011 --> 00:01:47,820
notion of [inaudible] that we saw when we
did math inferencing [inaudible] when we

23
00:01:47,820 --> 00:01:51,760
were computing single [inaudible]
assignment, here continuing in the

24
00:01:51,760 --> 00:01:56,396
[inaudible] of [inaudible] learning as a
[inaudible] difference, we're computing

25
00:01:56,396 --> 00:02:01,335
inference estimate of the parameters. So
concretely, how is map inference

26
00:02:01,335 --> 00:02:07,724
implemented in the context of MRF or CRF
learning? A very typical solution is to

27
00:02:07,724 --> 00:02:14,272
define a Gaussian distribution over each
parameter theta I separately. And that is

28
00:02:14,272 --> 00:02:24,559
usually a zero mean. Uni-variant Kelsian.
With some variants. Sigma squared. And the

29
00:02:24,559 --> 00:02:30,300
variants of sigma squared dictate how
firmly we believe that the parameter is

30
00:02:30,300 --> 00:02:36,263
close to zero. So for small variances, we
are very confident that the parameter is

31
00:02:36,263 --> 00:02:41,857
close to zero. And are going to be
unlikely to be swayed by a limited amount

32
00:02:41,857 --> 00:02:47,451
of data. Whereas, sigma gets larger, we're
going to be more inclined to belief,

33
00:02:47,451 --> 00:02:53,410
believe the data earlier on, and move the
parameter away from zero. So, we have such

34
00:02:53,410 --> 00:02:58,776
a parameter prior over each data I
sub-routines are multiplied together to

35
00:02:58,776 --> 00:03:04,073
give a joint parameter prior. So, the
parameter prior over each is going to, of

36
00:03:04,073 --> 00:03:09,501
each parameter is going to look like this.
This sigma squared. Is called a

37
00:03:09,501 --> 00:03:16,174
hyper-perameter. And it's exactly the same
kind of beast as we had for [inaudible]

38
00:03:16,174 --> 00:03:21,434
hyper parameters and the context of
learning [inaudible]. An alternative prior

39
00:03:21,434 --> 00:03:26,735
that's also in common use is what's called
the Laplacian parameter prior. And the

40
00:03:26,735 --> 00:03:32,101
Laplacian parameter prior looks kind of
similar to the Gaussian in that it has an

41
00:03:32,101 --> 00:03:36,813
exponential that increases as the
parameter moves away from zero. But in

42
00:03:36,813 --> 00:03:42,114
this case the increase in the parameter
depends on the absolute value of theta I

43
00:03:42,114 --> 00:03:47,481
and not on theta I squared, which is what,
the behavior that we would have with the

44
00:03:47,481 --> 00:03:54,855
Gaussian. And so this. Function looks, l,
looks as we see over here with a much

45
00:03:54,855 --> 00:04:01,048
sharper peak around zero that effectively
corresponds to a discontinuity at theta I

46
00:04:01,048 --> 00:04:06,871
equals, equals zero. And we have again
such a prior, a Laplacian prior theta I

47
00:04:06,871 --> 00:04:12,695
over each of the parameters of theta I
which are multiplied together. Just like

48
00:04:12,695 --> 00:04:18,371
the Gaussian, this distribution has a
hyper parameter, which in this case is

49
00:04:18,371 --> 00:04:23,767
often called beta. And the hyper
parameter, just like the variance. In the

50
00:04:23,767 --> 00:04:28,912
Gaussian distribution, sigma squared
dictates how tight this distribution is

51
00:04:28,912 --> 00:04:34,667
around zero where tighter distributions
correspond to cases where the model is

52
00:04:34,667 --> 00:04:39,881
going to be less inclined to move away
from zero based on limited amounts of

53
00:04:39,881 --> 00:04:45,905
data. So now, let's consider what map
estimation would look like in the context

54
00:04:45,905 --> 00:04:52,136
of these two distributions. So here, we
have these two parameter priors rewritten,

55
00:04:52,136 --> 00:04:58,212
the [inaudible] and the [inaudible]. And
now, map estimation corresponds to the

56
00:04:58,212 --> 00:05:04,209
[inaudible] over theta of the joint
distribution P of B comma theta. So we're

57
00:05:04,209 --> 00:05:10,440
trying to maximize, [inaudible], we're
trying to find the, theta that maximizes

58
00:05:10,440 --> 00:05:17,963
this joint distribution. And by the simple
rules of probability theory, this joint

59
00:05:17,963 --> 00:05:26,719
distribution is the product of the given
data, which is our likelihood. Multiplied

60
00:05:26,719 --> 00:05:33,260
the prior PSA value. And because of the
linearity of the [inaudible] not

61
00:05:33,260 --> 00:05:39,540
linearity, sorry, lognicity of the
logarithm function. We have the, this is

62
00:05:39,540 --> 00:05:46,605
the same as doing our maps of theta, of
our log likelihood function. Which is just

63
00:05:46,605 --> 00:05:53,963
the log of what we're doing. And the log
of the parameter prior. So, now let's look

64
00:05:53,963 --> 00:05:59,628
at what these logarithms look like in the
context of these two priors. And so this

65
00:05:59,628 --> 00:06:05,224
is, I'm actually drawing the negative log
of T of theta. And the negative log of T

66
00:06:05,224 --> 00:06:12,200
of theta for a Gaussian distribution is
simply a quadratic. And so we have a

67
00:06:12,200 --> 00:06:17,428
quadratic penalty that pushes the
parameters towards zero and this is the

68
00:06:17,428 --> 00:06:23,292
same for those of you who have seen this
in the machine learning class for example,

69
00:06:23,292 --> 00:06:30,207
as doing L1 regularization. Over the
parameter thing I the distribution if we

70
00:06:30,207 --> 00:06:37,267
now do the same for the [inaudible]
distribution, we're going to get. The a

71
00:06:37,267 --> 00:06:43,585
penalty that pushes the parameter towards
zero in a way that depends on the absolute

72
00:06:43,585 --> 00:06:49,011
value of the parameter. And this is
equivalent to L1 regularization, which

73
00:06:49,011 --> 00:06:54,510
again, some of you might have seen in a
machine learning class. So this is a

74
00:06:54,510 --> 00:06:59,509
linear penalty. Now these penalties
although hey both push the parameters

75
00:06:59,509 --> 00:07:04,877
towards zero. Am. Are actually quite
different in terms of the behavior. So if

76
00:07:04,877 --> 00:07:09,332
we look at what L2 does, in, when the
parameter is far away from zero, sort of

77
00:07:09,332 --> 00:07:14,138
towards the edges, the quadratic penalty's
actually quite severe, and it pushes the

78
00:07:14,138 --> 00:07:19,043
parameter down dramatically. But as the
parameter gets small, so closer to the

79
00:07:19,243 --> 00:07:24,505
case of theta I equals zero, the pressure
to move down decreases. So, effectively

80
00:07:24,505 --> 00:07:30,033
you, it's quite legitimate for the model
to have a lot of parameters that are close

81
00:07:30,033 --> 00:07:35,228
to zero but not quite zero. And there
isn't a lot of push at that point to get

82
00:07:35,228 --> 00:07:41,032
the parameters to hit zero exactly. And so
that means that models that use L2

83
00:07:41,032 --> 00:07:47,976
regularization are going to be dense. That
is, a lot of the Theta I's are going to be

84
00:07:47,976 --> 00:07:54,165
non-zero. Conversely when you look at the
one regularization you see that there is a

85
00:07:54,165 --> 00:07:59,639
consistent push towards zero regardless of
where in the parameter space we are. And

86
00:07:59,639 --> 00:08:04,850
so it's in the interest of the model to
continue pushing parameters that don't

87
00:08:04,850 --> 00:08:10,191
impact significantly the log likelihood
component and push them towards zero. And

88
00:08:10,191 --> 00:08:15,423
so, for L1 regularization, we're going to
get models that are sparse. And sparsity

89
00:08:15,423 --> 00:08:20,916
has its own value, because the sparser the
model, the easier it is to do inference in

90
00:08:20,916 --> 00:08:25,559
it, in general. Because sparsity
corresponds to in principle, removing

91
00:08:25,559 --> 00:08:30,594
edges or removing features from the
graphical model, which in turn, can remove

92
00:08:30,594 --> 00:08:35,303
edges and make inference more efficient.
And so, L1 regularization, or the

93
00:08:35,303 --> 00:08:40,142
corresponding Laplacian penalty, is often
used to make the model both more

94
00:08:40,142 --> 00:08:45,533
comprehensible, as well as computationally
more efficient. So to summarize, when

95
00:08:45,533 --> 00:08:51,522
we're doing learning in undirected models.
The parameter coupling that we have in the

96
00:08:51,522 --> 00:08:56,599
likelihood function prevents us from dong
Bayesian estimations efficiency that is

97
00:08:56,599 --> 00:09:01,551
maintaining a posture distribution over
parameters. However we can still use the

98
00:09:01,551 --> 00:09:06,628
parameter in [inaudible] parameter priors
to avoid the over fitting that we would

99
00:09:06,628 --> 00:09:11,643
get with maximum likely destination and
specifically we're going to use map as a

100
00:09:11,643 --> 00:09:17,169
substitute for MLE. The most typical
priors used in this context are L1 and L2,

101
00:09:17,169 --> 00:09:22,993
or Gaussian and McClaussian, and both of
these drive the parameters toward zero,

102
00:09:22,993 --> 00:09:28,670
which has the effect of preventing the
models from going wild and using very.

103
00:09:28,670 --> 00:09:34,258
Drastic parameter values that allow us to
over fit the specific training data that

104
00:09:34,258 --> 00:09:39,376
we've seen. Now, while this behavior is
shared among these two models, there's

105
00:09:39,376 --> 00:09:45,167
also a significant difference between them
in that the L1 or [inaudible] distribution

106
00:09:45,167 --> 00:09:50,823
can be shown both empirically and provably
to induce par solutions, and therefore it

107
00:09:50,823 --> 00:09:55,604
perform for us as forms of feature
selection or equivalently structure

108
00:09:55,604 --> 00:09:58,500
learning in the context of MRF's and
CRF's.
