
1
00:00:02,380 --> 00:00:06,606
So we previously defined the notion of
Bayesian estimation, and showed how it

2
00:00:06,606 --> 00:00:10,887
could be applied in the context of a
single random variable say, a multinomial

3
00:00:10,887 --> 00:00:15,004
random variable. Now we're going to step
back to the world of probabilistic

4
00:00:15,004 --> 00:00:19,340
graphical models and think about the
application of these ideas to the problem

5
00:00:19,340 --> 00:00:24,680
of estimating parameters in a Bayesian
network. So let's draw again the

6
00:00:24,680 --> 00:00:30,360
probabilistic graphical model that
represents Bayesian estimation in a

7
00:00:30,360 --> 00:00:37,000
Bayesian network. So just as before in a
single variable case we're going to inject

8
00:00:37,000 --> 00:00:43,800
into the model explicitly the parameters
that characterize sorry, random variables

9
00:00:43,800 --> 00:00:50,520
that define the parameters. And so here we
have two random variables theta X which

10
00:00:50,520 --> 00:00:55,798
represents the CPD of X. And theta Y given
X, which represents the CPD p of Y given

11
00:00:55,798 --> 00:01:00,374
X. Notice that each of these actually
vector valued because there's gonna be

12
00:01:00,374 --> 00:01:04,891
multiple actual numbers in each of these
CPDs, but we're going to draw them as

13
00:01:04,891 --> 00:01:10,718
single circles. Now, once again, we can
look at this network, and read out certain

14
00:01:10,718 --> 00:01:15,426
important conclusions. So the first
important conclusion is that the

15
00:01:15,426 --> 00:01:20,272
instances, these XY pairs, are
independent, given the parameters. And you

16
00:01:20,272 --> 00:01:25,327
can see that by noticing that if I
condition on both theta X and theta Y,

17
00:01:25,327 --> 00:01:30,311
given X, then the, XY pairs become
conditionally, become [inaudible]

18
00:01:30,311 --> 00:01:35,919
d-separated from each other. And so we have
conditional independence, following as a

19
00:01:35,919 --> 00:01:41,819
consequence of the structure of the
graphical model. We also have, and this is

20
00:01:41,819 --> 00:01:47,855
another explicit property that we can read
from this diagram, is that Theta-X and

21
00:01:47,855 --> 00:01:53,516
Theta-Y given X, are marginally
independent. So a priori we have that

22
00:01:53,516 --> 00:01:59,902
the parameter prior over all of the
parameters, theta, can be written as the

23
00:01:59,902 --> 00:02:06,534
product over I, in this case random I is
being the random variable in the network

24
00:02:06,534 --> 00:02:13,248
of the prior over the CPD for xi and so
the prior is the product of little priors,

25
00:02:13,248 --> 00:02:24,909
one for each CPD. Now, it follows from
this. From by just writing down the, the

26
00:02:24,909 --> 00:02:32,296
graphical model and looking at what the,
the implications of the expressions are

27
00:02:32,296 --> 00:02:39,505
the, the posteriors of the theta are also
independent given complete data. And the

28
00:02:39,505 --> 00:02:46,536
reason for that is that complete data
d-seperates the parameters for the two

29
00:02:46,536 --> 00:02:54,190
CPDs. So if you look at this network over
here and assume that we have all of these

30
00:02:54,190 --> 00:03:00,808
variables condition-, observed, then you
can see that there's no active path

31
00:03:00,808 --> 00:03:07,079
between theta x and theta y given x.
Because, for example, if we look at

32
00:03:07,079 --> 00:03:14,680
potentially this. Trail. We can see that
X2 blocks the trail from theta X to theta

33
00:03:14,680 --> 00:03:20,502
Y, given X. And so, again, following
directly from the structure of this

34
00:03:20,502 --> 00:03:26,831
network, we can see that the posterior
distribution, theta X, theta Y, given X

35
00:03:26,831 --> 00:03:33,581
again. given D decomposes as a product of
the posterior over theta X given D, times

36
00:03:33,581 --> 00:03:39,646
the posterior of theta Y, Y given X given
D. Which means just as in maximum likelihood

37
00:03:39,646 --> 00:03:44,992
estimation where it can break up the
estimation problem into one of estimating

38
00:03:44,992 --> 00:03:50,473
each cpd separately. We can do the same
here only now we can do it using Bayesian

39
00:03:50,473 --> 00:03:56,360
estimation where instead of just picking a
single parameter setting for, for the cpd

40
00:03:56,360 --> 00:04:01,774
we compute these separate posteriors
separately and then put them together into

41
00:04:01,774 --> 00:04:09,790
a single posterior. Now it turns out that
we could do even finer breakdown in the

42
00:04:09,790 --> 00:04:16,153
context of table CPDs. So here we have,
when we're now looking at the binary case

43
00:04:16,153 --> 00:04:22,674
where X is a binary value random variable.
So now we have two multi-nomials in our

44
00:04:22,674 --> 00:04:29,275
CPD, one for Y given X, one corresponding
to the case of Y given X1 and the other to

45
00:04:29,275 --> 00:04:36,722
Y given X0. And it turns out that if;
again in this model we're assuming that

46
00:04:36,722 --> 00:04:43,520
these are independent a priori. Which is
what this diagram says, because you notice

47
00:04:43,520 --> 00:04:48,593
that there's no edges between them, so
they're marginally independent, theta y

48
00:04:48,593 --> 00:04:53,864
given x one and theta y given x zero. If
we're postulating that, that holds, as we

49
00:04:53,864 --> 00:04:58,740
are in this diagram, it turns out that
they are also independent in the

50
00:04:58,740 --> 00:05:03,550
posterior. Now, that is a little bit
trickier to show, because it turns out

51
00:05:03,550 --> 00:05:08,557
that you can't read it directly from the
diagram, because in fact even given

52
00:05:08,557 --> 00:05:13,433
complete data, we have an active trail
that goes from. theta Y. Given X. One

53
00:05:13,433 --> 00:05:19,175
through Y. One. Which since it's observed
it activates a V. Structure. Instance

54
00:05:19,175 --> 00:05:27,122
theta Y given X0. But, it turns out. That
if we go back to some of the examples of

55
00:05:27,122 --> 00:05:33,408
context-specific independence that we had
in the case of specifically multiplexer

56
00:05:33,408 --> 00:05:39,621
CPD we can derive that these are in fact
despite the appearance of an activated V

57
00:05:39,621 --> 00:05:47,038
structure conditionally independent in the
posterior as well. And so once again, we

58
00:05:47,038 --> 00:05:56,551
can compute the posterior as a product of
posteriors of the form p of theta X given

59
00:05:56,551 --> 00:06:06,064
D times the probability of theta Y given
X1 given D. times the probability of theta

60
00:06:06,064 --> 00:06:14,274
one given X0. We can generalize this to a
general Bayesian network and let?s assume

61
00:06:14,274 --> 00:06:20,237
that we have a Bayesian network with table
CPD's that specified in terms of

62
00:06:20,237 --> 00:06:28,094
multinomial parameters of the form theta x
given u where u is some assignment to X's

63
00:06:28,094 --> 00:06:37,980
parents, U. Then if, for each such
multinomial parameter, we have dirichlet

64
00:06:37,980 --> 00:06:43,955
prior with appropriate hyper parameters.
Then we can show using the kind of

65
00:06:43,955 --> 00:06:50,569
analysis that we just did, combined with
the analysis of the posterior for a single

66
00:06:50,569 --> 00:06:56,624
multinomial. That the posterior is now a
dirichlet, with hyperparameters that

67
00:06:56,624 --> 00:07:02,779
represent the. Prior that we had for that
multinomial, plus the sufficient

68
00:07:02,779 --> 00:07:09,784
statistics that represent the count in the
data of particular combinations of the

69
00:07:09,784 --> 00:07:16,020
parent and the child. And so, for example,
for the entry in the multinomial

70
00:07:16,020 --> 00:07:22,682
representing the value little x1 for X,
and little u for the parents U. We have

71
00:07:22,682 --> 00:07:29,513
this. A prior parameter plus the counts
and the data for that combination of X and

72
00:07:29,513 --> 00:07:39,755
U. So. Now we know how to take a set of
priors and use the data to update them to

73
00:07:39,755 --> 00:07:45,785
form posteriors. Now, let's think about
where the priors might come from. And

74
00:07:45,785 --> 00:07:51,892
a priori it might seem very daunting, to
construct a set of priors for all of the

75
00:07:51,892 --> 00:07:58,318
nodes in a Bayesian network. It turns out
however that there is a general purpose

76
00:07:58,318 --> 00:08:05,115
scheme for doing that, that is both easy
and has some good theoretical properties.

77
00:08:05,115 --> 00:08:12,397
And that scheme works as follows. So what
we're going to do is we're going to define

78
00:08:12,397 --> 00:08:19,648
a prior Bayesian network that has some set
of parameters theta zero. And we're going

79
00:08:19,648 --> 00:08:27,831
to define a single equivalent sample size.
Which is going to be applicable or applied

80
00:08:27,831 --> 00:08:33,990
to all of the nodes in the Bayesian
network. And so, in order to specify the

81
00:08:33,990 --> 00:08:40,396
parameter, the hyperparameter alpha X
given U, for m, for an assignment X equals

82
00:08:40,396 --> 00:08:46,966
little x and U equals little u, we're
simply going to compute the probability in

83
00:08:46,966 --> 00:08:53,208
this parameterized network of X and U, and
we're going to multiply it by the

84
00:08:53,208 --> 00:09:00,023
equivalent sample size alpha. Now in many
cases you're just going to use theta zero

85
00:09:00,023 --> 00:09:06,967
to be the uniform parameters and which
makes it all a very easy computation. But

86
00:09:06,967 --> 00:09:13,241
this provides a simple coherent way to
specify all of the hyper-parameters

87
00:09:13,241 --> 00:09:23,181
simultaneously. And so let's look at an
example. Here is a network X Y that, that

88
00:09:23,181 --> 00:09:34,760
has no edge. And, and let's imagine that,
that is our That is our prior network. And

89
00:09:34,760 --> 00:09:43,646
so we're going to in fact assume a uniform
distribution, over the values, XY, and now

90
00:09:43,646 --> 00:09:52,690
let's. So Theta-0 is uniform. And now
let's look at what we would get for

91
00:09:52,690 --> 00:10:00,676
different situations in a network where we
have x being a parent of y. Which is the

92
00:10:00,676 --> 00:10:08,352
network whose parameters we actually want
to estimate. And so what we would get for,

93
00:10:08,352 --> 00:10:16,119
and let's assume they're both binary, x
and y. And so x is going to be distributed

94
00:10:16,119 --> 00:10:23,240
as a parameter with, Dirichlet with
hyperparameters alpha over two, alpha over

95
00:10:23,240 --> 00:10:30,946
two. And Y is going to be distributed.
Remember, so not [inaudible], not X. Theta

96
00:10:30,946 --> 00:10:38,986
of X is going to be distributed this way.
And theta of Y given XO is going to be

97
00:10:38,986 --> 00:10:46,725
distributed in the following way. It's a
dirichlet, with hyperparameters alpha

98
00:10:46,725 --> 00:10:56,935
times the probability of X, Y. Which in
the uniform distribution is a quarter. And

99
00:10:56,935 --> 00:11:10,080
similarly for theta y given x one is going
to have the same Dirichlet distribution.

100
00:11:10,080 --> 00:11:15,757
And if you think about this, this makes
perfect sense because it tells us that

101
00:11:15,757 --> 00:11:22,167
we've seen the same number of examples of
X. As we have of y. It's just that in y we

102
00:11:22,167 --> 00:11:28,912
have to partition the examples of x, of y
over those where we have x equals x0 and

103
00:11:28,912 --> 00:11:35,657
those we have x equals to x1. If on the
other hand, we had say Dirichlet of alpha

104
00:11:35,657 --> 00:11:41,496
over two, alpha over two for the two
except for the two multinomial?s

105
00:11:41,496 --> 00:11:48,260
corresponding to, to the corresponding to
y. This one and this one. It would rep, it

106
00:11:48,260 --> 00:11:56,383
would, imply that we've seen twice as many
Y's as we've seen X's. So let's see what

107
00:11:56,383 --> 00:12:02,465
kind of affect using the Bayesian
estimation has in a pseudo real world

108
00:12:02,465 --> 00:12:07,883
example. So this is actually a real
network. It was developed for monitoring

109
00:12:07,883 --> 00:12:13,664
patients in an ICU. And we call it the ICU
alarm network. And it turns out that the

110
00:12:13,664 --> 00:12:18,741
ICU alarm network has 37 different
variables. They represent things like

111
00:12:18,741 --> 00:12:24,804
whether the patient was [inaudible]. The
patient's blood pressure, heart rate, and

112
00:12:24,804 --> 00:12:30,444
various other medical, events that might
happen. And it turns out that, overall,

113
00:12:30,444 --> 00:12:35,658
the network has 504 parameters. Now there
aren't actually data cases here. This was

114
00:12:35,658 --> 00:12:40,433
a hand constructed network. And so what
we're going to do is we're going to sample

115
00:12:40,433 --> 00:12:45,208
instances from the network and then we're
going to pretend that we don't know the

116
00:12:45,208 --> 00:12:49,518
network parameters and see the extent to
which we can recover the network

117
00:12:49,518 --> 00:12:56,120
parameters via learning from the instances
that we sampled from it. Now I should say

118
00:12:56,120 --> 00:13:01,324
that this is. A pseudo realistic learning
problem. Because the instances that one

119
00:13:01,324 --> 00:13:06,421
samples from a network, are us, are always
cleaner than the instances that one gets

120
00:13:06,421 --> 00:13:10,958
in the context of a real world data
[inaudible], data set. Because it, in a

121
00:13:10,958 --> 00:13:16,303
real world scenario, it is rarely the case
that the network whose structure you, the

122
00:13:16,303 --> 00:13:21,649
network whose, that you're trying to learn
has the exact same structure as the true

123
00:13:21,649 --> 00:13:26,372
underlying distribution, from which the
data were generated. And so, this is a

124
00:13:26,372 --> 00:13:31,647
much cleaner scenario. But still, it's
useful and indicative. So what we see here

125
00:13:31,647 --> 00:13:37,997
are the results of learning as a function
of the x axis, which is the number of

126
00:13:37,997 --> 00:13:44,321
samples. And the y-axis is a distant
function between the true distribution and

127
00:13:44,321 --> 00:13:49,503
the learned distribution. And that distance
function we're not going to talk about

128
00:13:49,503 --> 00:13:55,196
this at the moment it's the notion called
relative entropy. It's also called kale

129
00:13:55,196 --> 00:14:02,001
divergence. But what we need to know about
this for the purpose of the current

130
00:14:02,001 --> 00:14:07,160
discussion, is that when two distributions
are identical it's zero, and otherwise

131
00:14:07,160 --> 00:14:12,822
it's non-negative. So what we see here is
that the blue line Corresponds to maximum

132
00:14:12,822 --> 00:14:18,612
likelihood estimation. And we can see several
things about the blue line. First of all

133
00:14:18,612 --> 00:14:23,730
it's very jagged. There is a lot of bumps
in it. And second, it's consistently

134
00:14:23,730 --> 00:14:29,252
higher than all of the other lines. Which
means that maximum likelihood estimation,

135
00:14:29,252 --> 00:14:34,707
although it does continue to get lower as
we get more data, with as high as 5,000

136
00:14:34,707 --> 00:14:39,960
data points, we still haven't gotten close
to the true underlying distribution.

137
00:14:39,960 --> 00:14:45,687
Conversely, let's see what happens with
the Bayesian estimation and this is all

138
00:14:45,687 --> 00:14:52,561
Bayesian estimation with a uniform prior.
And different equivalent sample size. So

139
00:14:52,561 --> 00:14:57,541
this is using the prior network with a
uniform network and different values of

140
00:14:57,541 --> 00:15:05,750
alpha. And what we see here is that for
alpha equals five, that's the green line.

141
00:15:05,750 --> 00:15:11,344
Alpha equal ten are almost sitting
directly on top of each other. And they're

142
00:15:11,344 --> 00:15:17,157
both considerably lower than all of the
other lines and also the maximum likelihood

143
00:15:17,157 --> 00:15:22,607
estimation. As we increase the prior
strength so that we are, have a firmer

144
00:15:22,607 --> 00:15:27,685
belief in, in. An uniform prior. We can
see that we move a little bit away. And

145
00:15:27,685 --> 00:15:32,596
now the performance becomes a little
worse. But notice that by around 2000 data

146
00:15:32,596 --> 00:15:38,003
points, we're already pretty close to the
case that we were for an equivalent sample

147
00:15:38,003 --> 00:15:43,038
size of five. For 50, which is this dark
blue line, it takes a little bit longer to

148
00:15:43,038 --> 00:15:48,010
converge, and it doesn't quite make it.
But even with a, an equivalent sample size

149
00:15:48,010 --> 00:15:52,858
of 50, which is pretty high you get
convergence to the correct distribution

150
00:15:52,858 --> 00:15:57,671
much faster than you do from maximum
likelihood estimation. So to summarize,

151
00:15:57,863 --> 00:16:02,651
in Bayesian networks if we're doing
Bayesian parameter estimation, if we're

152
00:16:02,651 --> 00:16:07,759
willing to stipulate the parameters are
independent a priori, then they're

153
00:16:07,759 --> 00:16:12,931
also independent in the posterior which
allows us to maintain the posterior as a

154
00:16:12,931 --> 00:16:17,627
product of posteriors over individual
parameters. For multinomial Bayesian

155
00:16:17,627 --> 00:16:22,750
networks, we can go ahead and con, perform
Bayesian estimation using the exact same

156
00:16:22,750 --> 00:16:27,030
sufficient statistics that we use for
maximum likelihood estimation. Which are the counts

157
00:16:27,030 --> 00:16:32,033
corresponding to a value of the variable
and a value of the parents and whereas in

158
00:16:32,033 --> 00:16:37,036
the context of maximum likelihood estimation you
would simply use the formula on the left,

159
00:16:37,036 --> 00:16:41,798
in the case of Bayesian estimation we're
going to use the formula on the right

160
00:16:41,798 --> 00:16:46,560
which has exactly the same form, only it
also accounts for the hyper parameters.

161
00:16:47,120 --> 00:16:52,611
And, in order to do this kind of process,
we need a choice of prior, and we've

162
00:16:52,611 --> 00:16:58,102
shown how that can be effectively elicited
using both a [inaudible] distributions

163
00:16:58,102 --> 00:17:02,589
specified as a Bayesian network, as well as
an equivalent sample size.
